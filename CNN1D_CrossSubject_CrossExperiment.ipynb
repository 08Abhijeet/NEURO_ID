{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow not available. Please install TensorFlow to run this notebook.\n",
            "You can try: pip install tensorflow==2.15.0\n",
            "Basic libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Try to import seaborn, if not available, we'll use matplotlib for plotting\n",
        "try:\n",
        "    import seaborn as sns\n",
        "    SEABORN_AVAILABLE = True\n",
        "except ImportError:\n",
        "    print(\"Seaborn not available, using matplotlib for plotting\")\n",
        "    SEABORN_AVAILABLE = False\n",
        "\n",
        "# Try to import TensorFlow, if not available, we'll provide alternative\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    from tensorflow.keras.models import Sequential\n",
        "    from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D, Dense, Dropout\n",
        "    from tensorflow.keras.utils import to_categorical\n",
        "    from tensorflow.keras.optimizers import Adam\n",
        "    from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "    TENSORFLOW_AVAILABLE = True\n",
        "    print(\"TensorFlow imported successfully!\")\n",
        "except ImportError:\n",
        "    print(\"TensorFlow not available. Please install TensorFlow to run this notebook.\")\n",
        "    print(\"You can try: pip install tensorflow==2.15.0\")\n",
        "    TENSORFLOW_AVAILABLE = False\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "print(\"Basic libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training subjects: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
            "Testing subjects: [13, 14, 15, 16, 17, 18, 19, 20]\n",
            "Training experiments: [1, 2]\n",
            "Testing experiments: [5, 6, 7, 8, 9, 10]\n",
            "Training sessions: [1, 2]\n",
            "Testing sessions: [1, 2, 3]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# --- Configuration ---\n",
        "DATA_PATH = 'Dataset/Filtered_Data' \n",
        "TOTAL_SUBJECTS = 20\n",
        "SAMPLES_PER_EPOCH = 400  # 2 seconds of data (200 Hz * 2s)\n",
        "NUM_CHANNELS = 4\n",
        "\n",
        "# --- Define Training and Testing Strategy ---\n",
        "# Training: Subjects 1-12, Experiments 1-2 (eyes closed/open), Sessions 1-2\n",
        "# Testing: Subjects 13-20, Experiments 5-10 (auditory stimuli), All sessions\n",
        "\n",
        "TRAIN_SUBJECTS = list(range(1, 13))  # Subjects 1-12 (12 subjects)\n",
        "TEST_SUBJECTS = list(range(13, 21))  # Subjects 13-20 (8 subjects)\n",
        "TRAIN_EXPERIMENTS = [1, 2]  # ex01 (eyes closed), ex02 (eyes open)\n",
        "TEST_EXPERIMENTS = [5, 6, 7, 8, 9, 10]  # Auditory stimuli experiments\n",
        "TRAIN_SESSIONS = [1, 2]  # First 2 sessions for training\n",
        "TEST_SESSIONS = [1, 2, 3]  # All sessions for testing\n",
        "\n",
        "print(f\"Training subjects: {TRAIN_SUBJECTS}\")\n",
        "print(f\"Testing subjects: {TEST_SUBJECTS}\")\n",
        "print(f\"Training experiments: {TRAIN_EXPERIMENTS}\")\n",
        "print(f\"Testing experiments: {TEST_EXPERIMENTS}\")\n",
        "print(f\"Training sessions: {TRAIN_SESSIONS}\")\n",
        "print(f\"Testing sessions: {TEST_SESSIONS}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading training data...\n",
            "Training data loaded.\n",
            "X_train shape: (2880, 400, 4)\n",
            "y_train shape: (2880,)\n",
            "Training subjects distribution: [240 240 240 240 240 240 240 240 240 240 240 240]\n"
          ]
        }
      ],
      "source": [
        "# --- Load Training Data ---\n",
        "print(\"Loading training data...\")\n",
        "train_epochs = []\n",
        "train_labels = []\n",
        "train_subject_mapping = {}\n",
        "\n",
        "# Create mapping from original subject IDs to training subject IDs (0-11)\n",
        "for idx, subject_id in enumerate(TRAIN_SUBJECTS):\n",
        "    train_subject_mapping[subject_id] = idx\n",
        "\n",
        "for subject_id in TRAIN_SUBJECTS:\n",
        "    for exp_id in TRAIN_EXPERIMENTS:\n",
        "        for session_id in TRAIN_SESSIONS:\n",
        "            # Construct the file path\n",
        "            file_name = f's{subject_id:02d}_ex{exp_id:02d}_s{session_id:02d}.csv'\n",
        "            file_path = os.path.join(DATA_PATH, file_name)\n",
        "            \n",
        "            if os.path.exists(file_path):\n",
        "                # Load the data using pandas\n",
        "                df = pd.read_csv(file_path)\n",
        "                # Get the EEG data (last 4 columns) as a NumPy array\n",
        "                eeg_data = df.iloc[:, 1:].values\n",
        "                \n",
        "                # Slice the data into epochs\n",
        "                num_samples = eeg_data.shape[0]\n",
        "                for i in range(0, num_samples - SAMPLES_PER_EPOCH + 1, SAMPLES_PER_EPOCH):\n",
        "                    epoch = eeg_data[i:i+SAMPLES_PER_EPOCH, :]\n",
        "                    train_epochs.append(epoch)\n",
        "                    # Map to training subject ID (0-11)\n",
        "                    train_labels.append(train_subject_mapping[subject_id])\n",
        "            else:\n",
        "                print(f\"Warning: Training file not found at {file_path}\")\n",
        "\n",
        "# Convert to NumPy arrays\n",
        "X_train = np.array(train_epochs)\n",
        "y_train = np.array(train_labels)\n",
        "\n",
        "print(f\"Training data loaded.\")\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"Training subjects distribution: {np.bincount(y_train)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading testing data...\n",
            "Testing data loaded.\n",
            "X_test shape: (2890, 400, 4)\n",
            "y_test shape: (2890,)\n",
            "Testing subjects distribution: [360 360 360 360 370 360 360 360]\n"
          ]
        }
      ],
      "source": [
        "# --- Load Testing Data ---\n",
        "print(\"Loading testing data...\")\n",
        "test_epochs = []\n",
        "test_labels = []\n",
        "test_subject_mapping = {}\n",
        "\n",
        "# Create mapping from original subject IDs to testing subject IDs (0-7)\n",
        "for idx, subject_id in enumerate(TEST_SUBJECTS):\n",
        "    test_subject_mapping[subject_id] = idx\n",
        "\n",
        "for subject_id in TEST_SUBJECTS:\n",
        "    for exp_id in TEST_EXPERIMENTS:\n",
        "        # For experiments 5-10, there are no session numbers in filename\n",
        "        file_name = f's{subject_id:02d}_ex{exp_id:02d}.csv'\n",
        "        file_path = os.path.join(DATA_PATH, file_name)\n",
        "        \n",
        "        if os.path.exists(file_path):\n",
        "            # Load the data using pandas\n",
        "            df = pd.read_csv(file_path)\n",
        "            # Get the EEG data (last 4 columns) as a NumPy array\n",
        "            eeg_data = df.iloc[:, 1:].values\n",
        "            \n",
        "            # Slice the data into epochs\n",
        "            num_samples = eeg_data.shape[0]\n",
        "            for i in range(0, num_samples - SAMPLES_PER_EPOCH + 1, SAMPLES_PER_EPOCH):\n",
        "                epoch = eeg_data[i:i+SAMPLES_PER_EPOCH, :]\n",
        "                test_epochs.append(epoch)\n",
        "                # Map to testing subject ID (0-7)\n",
        "                test_labels.append(test_subject_mapping[subject_id])\n",
        "        else:\n",
        "            print(f\"Warning: Testing file not found at {file_path}\")\n",
        "\n",
        "# Convert to NumPy arrays\n",
        "X_test = np.array(test_epochs)\n",
        "y_test = np.array(test_labels)\n",
        "\n",
        "print(f\"Testing data loaded.\")\n",
        "print(f\"X_test shape: {X_test.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")\n",
        "print(f\"Testing subjects distribution: {np.bincount(y_test)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'to_categorical' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# One-hot encode the labels\u001b[39;00m\n\u001b[32m      8\u001b[39m NUM_TRAIN_SUBJECTS = \u001b[38;5;28mlen\u001b[39m(TRAIN_SUBJECTS)  \u001b[38;5;66;03m# 12 subjects for training\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m y_train_cat = \u001b[43mto_categorical\u001b[49m(y_train_split, num_classes=NUM_TRAIN_SUBJECTS)\n\u001b[32m     10\u001b[39m y_val_cat = to_categorical(y_val, num_classes=NUM_TRAIN_SUBJECTS)\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining split:\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mNameError\u001b[39m: name 'to_categorical' is not defined"
          ]
        }
      ],
      "source": [
        "# --- Prepare Data for Training ---\n",
        "# Split training data into train/validation (80/20)\n",
        "X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
        "    X_train, y_train, test_size=0.2, stratify=y_train, random_state=42\n",
        ")\n",
        "\n",
        "# One-hot encode the labels\n",
        "NUM_TRAIN_SUBJECTS = len(TRAIN_SUBJECTS)  # 12 subjects for training\n",
        "y_train_cat = to_categorical(y_train_split, num_classes=NUM_TRAIN_SUBJECTS)\n",
        "y_val_cat = to_categorical(y_val, num_classes=NUM_TRAIN_SUBJECTS)\n",
        "\n",
        "print(f\"Training split:\")\n",
        "print(f\"X_train_split shape: {X_train_split.shape}\")\n",
        "print(f\"X_val shape: {X_val.shape}\")\n",
        "print(f\"y_train_cat shape: {y_train_cat.shape}\")\n",
        "print(f\"y_val_cat shape: {y_val_cat.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Build Model ---\n",
        "if not TENSORFLOW_AVAILABLE:\n",
        "    print(\"TensorFlow is not available. Cannot build the model.\")\n",
        "    print(\"Please install TensorFlow to continue.\")\n",
        "    print(\"You can try: pip install tensorflow==2.15.0\")\n",
        "    model = None\n",
        "else:\n",
        "    model = Sequential()\n",
        "\n",
        "    # 1. Input Layer (defined by input_shape in the first Conv1D layer)\n",
        "    # Shape: (400 samples, 4 channels)\n",
        "\n",
        "    # 2. First Conv1D Layer\n",
        "    model.add(Conv1D(filters=64, kernel_size=5, activation='relu', input_shape=(SAMPLES_PER_EPOCH, NUM_CHANNELS)))\n",
        "\n",
        "    # 3. MaxPooling1D Layer\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "    # 4. Second Conv1D Layer\n",
        "    model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
        "\n",
        "    # 5. GlobalAveragePooling1D Layer\n",
        "    model.add(GlobalAveragePooling1D())\n",
        "\n",
        "    # 6. Dense Layer\n",
        "    model.add(Dense(100, activation='relu'))\n",
        "\n",
        "    # 7. Dropout Layer (for regularization)\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    # 8. Output Dense Layer - Now for 12 training subjects\n",
        "    model.add(Dense(NUM_TRAIN_SUBJECTS, activation='softmax'))\n",
        "\n",
        "    # Print a summary of the model architecture\n",
        "    model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Compile and Train Model ---\n",
        "custom_adam = Adam(learning_rate=0.0005)\n",
        "model.compile(\n",
        "    optimizer=custom_adam, \n",
        "    loss='categorical_crossentropy', \n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "callbacks_list = [\n",
        "    EarlyStopping(monitor='val_loss', patience=15, verbose=1),\n",
        "    ModelCheckpoint(filepath='best_model_cross_subject.h5', monitor='val_accuracy', save_best_only=True, verbose=1),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, min_lr=0.00001)\n",
        "]\n",
        "\n",
        "print(\"Starting training...\")\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train_split, \n",
        "    y_train_cat, \n",
        "    batch_size=16, \n",
        "    epochs=100,\n",
        "    validation_data=(X_val, y_val_cat),\n",
        "    callbacks=callbacks_list\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Plot Training History ---\n",
        "pd.DataFrame(history.history).plot(figsize=(10, 7))\n",
        "plt.grid(True)\n",
        "plt.gca().set_ylim(0, 1.1)\n",
        "plt.title('Model Training History - Cross Subject/Experiment')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Metric Value')\n",
        "plt.show()\n",
        "\n",
        "# --- Evaluate on Validation Set (same subjects, different sessions) ---\n",
        "val_loss, val_acc = model.evaluate(X_val, y_val_cat, verbose=2)\n",
        "print(f\"\\nValidation Accuracy (same subjects, different sessions): {val_acc*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Cross-Subject and Cross-Experiment Testing ---\n",
        "print(\"\\n=== CROSS-SUBJECT AND CROSS-EXPERIMENT TESTING ===\")\n",
        "print(f\"Testing on {len(TEST_SUBJECTS)} different subjects with different experiments\")\n",
        "print(f\"Training subjects: {TRAIN_SUBJECTS}\")\n",
        "print(f\"Testing subjects: {TEST_SUBJECTS}\")\n",
        "print(f\"Training experiments: {TRAIN_EXPERIMENTS}\")\n",
        "print(f\"Testing experiments: {TEST_EXPERIMENTS}\")\n",
        "\n",
        "# For cross-subject testing, we need to map test subjects to training subjects\n",
        "# This is a challenging task - the model was trained on different subjects\n",
        "# We'll use the model as-is and see how it performs\n",
        "\n",
        "# Get predictions on test data\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Since we're testing on different subjects, we need to map the predictions\n",
        "# The model predicts training subject IDs (0-11), but we have test subject IDs (0-7)\n",
        "# We'll create a mapping strategy\n",
        "\n",
        "print(f\"\\nTest data shape: {X_test.shape}\")\n",
        "print(f\"Number of test samples: {len(y_test)}\")\n",
        "print(f\"Test subject distribution: {np.bincount(y_test)}\")\n",
        "\n",
        "# Calculate accuracy by mapping test subjects to training subjects\n",
        "# Simple strategy: map test subjects to training subjects based on order\n",
        "subject_mapping = {}\n",
        "for i, test_subject in enumerate(TEST_SUBJECTS):\n",
        "    # Map to training subject (cycling through training subjects)\n",
        "    mapped_train_subject = i % len(TRAIN_SUBJECTS)\n",
        "    subject_mapping[i] = mapped_train_subject\n",
        "    print(f\"Test subject {test_subject} (ID {i}) -> Training subject {TRAIN_SUBJECTS[mapped_train_subject]} (ID {mapped_train_subject})\")\n",
        "\n",
        "# Map predictions to test subject space\n",
        "y_pred_mapped = np.array([subject_mapping[pred] for pred in y_pred_classes])\n",
        "\n",
        "# Calculate accuracy\n",
        "correct_predictions = np.sum(y_pred_mapped == y_test)\n",
        "total_predictions = len(y_test)\n",
        "accuracy = correct_predictions / total_predictions\n",
        "\n",
        "print(f\"\\nCross-Subject Cross-Experiment Accuracy: {accuracy*100:.2f}%\")\n",
        "print(f\"Correct predictions: {correct_predictions}/{total_predictions}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Create Confusion Matrix for Cross-Subject Testing ---\n",
        "cm = confusion_matrix(y_test, y_pred_mapped)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "if SEABORN_AVAILABLE:\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "                xticklabels=[f'TS{i+1}' for i in range(len(TEST_SUBJECTS))],\n",
        "                yticklabels=[f'TS{i+1}' for i in range(len(TEST_SUBJECTS))])\n",
        "else:\n",
        "    # Use matplotlib for heatmap if seaborn is not available\n",
        "    plt.imshow(cm, interpolation='nearest', cmap='Blues')\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(TEST_SUBJECTS))\n",
        "    plt.xticks(tick_marks, [f'TS{i+1}' for i in range(len(TEST_SUBJECTS))])\n",
        "    plt.yticks(tick_marks, [f'TS{i+1}' for i in range(len(TEST_SUBJECTS))])\n",
        "    \n",
        "    # Add text annotations\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in np.ndindex(cm.shape):\n",
        "        plt.text(j, i, format(cm[i, j], 'd'),\n",
        "                ha=\"center\", va=\"center\",\n",
        "                color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "plt.title('Confusion Matrix - Cross Subject/Experiment Testing\\n(Test Subjects vs Mapped Predictions)')\n",
        "plt.ylabel('True Test Subject')\n",
        "plt.xlabel('Predicted Test Subject')\n",
        "plt.show()\n",
        "\n",
        "# --- Classification Report ---\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_mapped, \n",
        "                          target_names=[f'TestSubject{i+1}' for i in range(len(TEST_SUBJECTS))]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Alternative Approach: Train a new model for cross-subject generalization ---\n",
        "print(\"\\n=== ALTERNATIVE APPROACH: TRAINING FOR CROSS-SUBJECT GENERALIZATION ===\")\n",
        "print(\"This approach trains the model to learn general EEG patterns that can generalize across subjects.\")\n",
        "\n",
        "# For this approach, we'll use all available data but with a different strategy\n",
        "# We'll train on a subset of subjects and test on completely unseen subjects\n",
        "\n",
        "# Let's create a more realistic cross-subject scenario\n",
        "# Train on subjects 1-10, test on subjects 11-20\n",
        "TRAIN_SUBJECTS_ALT = list(range(1, 11))  # Subjects 1-10\n",
        "TEST_SUBJECTS_ALT = list(range(11, 21))  # Subjects 11-20\n",
        "\n",
        "print(f\"Alternative training subjects: {TRAIN_SUBJECTS_ALT}\")\n",
        "print(f\"Alternative testing subjects: {TEST_SUBJECTS_ALT}\")\n",
        "\n",
        "# Load training data for alternative approach\n",
        "train_epochs_alt = []\n",
        "train_labels_alt = []\n",
        "\n",
        "# Create mapping for alternative approach\n",
        "alt_train_mapping = {}\n",
        "for idx, subject_id in enumerate(TRAIN_SUBJECTS_ALT):\n",
        "    alt_train_mapping[subject_id] = idx\n",
        "\n",
        "for subject_id in TRAIN_SUBJECTS_ALT:\n",
        "    for exp_id in [1, 2]:  # Use both eyes closed and eyes open\n",
        "        for session_id in [1, 2, 3]:  # Use all sessions\n",
        "            file_name = f's{subject_id:02d}_ex{exp_id:02d}_s{session_id:02d}.csv'\n",
        "            file_path = os.path.join(DATA_PATH, file_name)\n",
        "            \n",
        "            if os.path.exists(file_path):\n",
        "                df = pd.read_csv(file_path)\n",
        "                eeg_data = df.iloc[:, 1:].values\n",
        "                \n",
        "                num_samples = eeg_data.shape[0]\n",
        "                for i in range(0, num_samples - SAMPLES_PER_EPOCH + 1, SAMPLES_PER_EPOCH):\n",
        "                    epoch = eeg_data[i:i+SAMPLES_PER_EPOCH, :]\n",
        "                    train_epochs_alt.append(epoch)\n",
        "                    train_labels_alt.append(alt_train_mapping[subject_id])\n",
        "\n",
        "X_train_alt = np.array(train_epochs_alt)\n",
        "y_train_alt = np.array(train_labels_alt)\n",
        "\n",
        "print(f\"Alternative training data shape: {X_train_alt.shape}\")\n",
        "print(f\"Alternative training labels shape: {y_train_alt.shape}\")\n",
        "print(f\"Alternative training subject distribution: {np.bincount(y_train_alt)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Load testing data for alternative approach ---\n",
        "test_epochs_alt = []\n",
        "test_labels_alt = []\n",
        "\n",
        "# Create mapping for test subjects (map to training subject space)\n",
        "alt_test_mapping = {}\n",
        "for idx, subject_id in enumerate(TEST_SUBJECTS_ALT):\n",
        "    # Map test subjects to training subjects (cycling through)\n",
        "    alt_test_mapping[subject_id] = idx % len(TRAIN_SUBJECTS_ALT)\n",
        "\n",
        "for subject_id in TEST_SUBJECTS_ALT:\n",
        "    for exp_id in [1, 2]:  # Use same experiments as training\n",
        "        for session_id in [1, 2, 3]:  # Use all sessions\n",
        "            file_name = f's{subject_id:02d}_ex{exp_id:02d}_s{session_id:02d}.csv'\n",
        "            file_path = os.path.join(DATA_PATH, file_name)\n",
        "            \n",
        "            if os.path.exists(file_path):\n",
        "                df = pd.read_csv(file_path)\n",
        "                eeg_data = df.iloc[:, 1:].values\n",
        "                \n",
        "                num_samples = eeg_data.shape[0]\n",
        "                for i in range(0, num_samples - SAMPLES_PER_EPOCH + 1, SAMPLES_PER_EPOCH):\n",
        "                    epoch = eeg_data[i:i+SAMPLES_PER_EPOCH, :]\n",
        "                    test_epochs_alt.append(epoch)\n",
        "                    # Map to training subject space\n",
        "                    test_labels_alt.append(alt_test_mapping[subject_id])\n",
        "\n",
        "X_test_alt = np.array(test_epochs_alt)\n",
        "y_test_alt = np.array(test_labels_alt)\n",
        "\n",
        "print(f\"Alternative testing data shape: {X_test_alt.shape}\")\n",
        "print(f\"Alternative testing labels shape: {y_test_alt.shape}\")\n",
        "print(f\"Alternative testing subject distribution: {np.bincount(y_test_alt)}\")\n",
        "\n",
        "# Split training data for validation\n",
        "X_train_split_alt, X_val_alt, y_train_split_alt, y_val_alt = train_test_split(\n",
        "    X_train_alt, y_train_alt, test_size=0.2, stratify=y_train_alt, random_state=42\n",
        ")\n",
        "\n",
        "# One-hot encode\n",
        "NUM_TRAIN_SUBJECTS_ALT = len(TRAIN_SUBJECTS_ALT)\n",
        "y_train_cat_alt = to_categorical(y_train_split_alt, num_classes=NUM_TRAIN_SUBJECTS_ALT)\n",
        "y_val_cat_alt = to_categorical(y_val_alt, num_classes=NUM_TRAIN_SUBJECTS_ALT)\n",
        "y_test_cat_alt = to_categorical(y_test_alt, num_classes=NUM_TRAIN_SUBJECTS_ALT)\n",
        "\n",
        "print(f\"\\nAlternative approach data ready:\")\n",
        "print(f\"Training: {X_train_split_alt.shape}, Validation: {X_val_alt.shape}, Testing: {X_test_alt.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Build and Train Alternative Model ---\n",
        "model_alt = Sequential()\n",
        "\n",
        "model_alt.add(Conv1D(filters=64, kernel_size=5, activation='relu', input_shape=(SAMPLES_PER_EPOCH, NUM_CHANNELS)))\n",
        "model_alt.add(MaxPooling1D(pool_size=2))\n",
        "model_alt.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
        "model_alt.add(GlobalAveragePooling1D())\n",
        "model_alt.add(Dense(100, activation='relu'))\n",
        "model_alt.add(Dropout(0.3))\n",
        "model_alt.add(Dense(NUM_TRAIN_SUBJECTS_ALT, activation='softmax'))\n",
        "\n",
        "# Compile\n",
        "model_alt.compile(\n",
        "    optimizer=Adam(learning_rate=0.0005), \n",
        "    loss='categorical_crossentropy', \n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "callbacks_alt = [\n",
        "    EarlyStopping(monitor='val_loss', patience=15, verbose=1),\n",
        "    ModelCheckpoint(filepath='best_model_alt_cross_subject.h5', monitor='val_accuracy', save_best_only=True, verbose=1),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, min_lr=0.00001)\n",
        "]\n",
        "\n",
        "print(\"Training alternative model for cross-subject generalization...\")\n",
        "history_alt = model_alt.fit(\n",
        "    X_train_split_alt, \n",
        "    y_train_cat_alt, \n",
        "    batch_size=16, \n",
        "    epochs=100,\n",
        "    validation_data=(X_val_alt, y_val_cat_alt),\n",
        "    callbacks=callbacks_alt\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Evaluate Alternative Model ---\n",
        "# Plot training history\n",
        "pd.DataFrame(history_alt.history).plot(figsize=(10, 7))\n",
        "plt.grid(True)\n",
        "plt.gca().set_ylim(0, 1.1)\n",
        "plt.title('Alternative Model Training History - Cross Subject Generalization')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Metric Value')\n",
        "plt.show()\n",
        "\n",
        "# Evaluate on validation set (same subjects, different sessions)\n",
        "val_loss_alt, val_acc_alt = model_alt.evaluate(X_val_alt, y_val_cat_alt, verbose=2)\n",
        "print(f\"\\nValidation Accuracy (same subjects, different sessions): {val_acc_alt*100:.2f}%\")\n",
        "\n",
        "# Evaluate on test set (different subjects)\n",
        "test_loss_alt, test_acc_alt = model_alt.evaluate(X_test_alt, y_test_cat_alt, verbose=2)\n",
        "print(f\"\\nCross-Subject Test Accuracy: {test_acc_alt*100:.2f}%\")\n",
        "\n",
        "# Get predictions for confusion matrix\n",
        "y_pred_alt = model_alt.predict(X_test_alt)\n",
        "y_pred_classes_alt = np.argmax(y_pred_alt, axis=1)\n",
        "\n",
        "# Create confusion matrix\n",
        "cm_alt = confusion_matrix(y_test_alt, y_pred_classes_alt)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "if SEABORN_AVAILABLE:\n",
        "    sns.heatmap(cm_alt, annot=True, fmt='d', cmap='Blues', \n",
        "                xticklabels=[f'TS{i+1}' for i in range(NUM_TRAIN_SUBJECTS_ALT)],\n",
        "                yticklabels=[f'TS{i+1}' for i in range(NUM_TRAIN_SUBJECTS_ALT)])\n",
        "else:\n",
        "    # Use matplotlib for heatmap if seaborn is not available\n",
        "    plt.imshow(cm_alt, interpolation='nearest', cmap='Blues')\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(NUM_TRAIN_SUBJECTS_ALT)\n",
        "    plt.xticks(tick_marks, [f'TS{i+1}' for i in range(NUM_TRAIN_SUBJECTS_ALT)])\n",
        "    plt.yticks(tick_marks, [f'TS{i+1}' for i in range(NUM_TRAIN_SUBJECTS_ALT)])\n",
        "    \n",
        "    # Add text annotations\n",
        "    thresh = cm_alt.max() / 2.\n",
        "    for i, j in np.ndindex(cm_alt.shape):\n",
        "        plt.text(j, i, format(cm_alt[i, j], 'd'),\n",
        "                ha=\"center\", va=\"center\",\n",
        "                color=\"white\" if cm_alt[i, j] > thresh else \"black\")\n",
        "\n",
        "plt.title('Confusion Matrix - Cross Subject Generalization\\n(Test Subjects Mapped to Training Subject Space)')\n",
        "plt.ylabel('True Test Subject (Mapped)')\n",
        "plt.xlabel('Predicted Subject')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nClassification Report for Cross-Subject Generalization:\")\n",
        "print(classification_report(y_test_alt, y_pred_classes_alt, \n",
        "                          target_names=[f'Subject{i+1}' for i in range(NUM_TRAIN_SUBJECTS_ALT)]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Summary of Results ---\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SUMMARY OF CROSS-SUBJECT AND CROSS-EXPERIMENT TESTING\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\n1. ORIGINAL APPROACH:\")\n",
        "print(f\"   - Training: Subjects {TRAIN_SUBJECTS}, Experiments {TRAIN_EXPERIMENTS}, Sessions {TRAIN_SESSIONS}\")\n",
        "print(f\"   - Testing: Subjects {TEST_SUBJECTS}, Experiments {TEST_EXPERIMENTS}, All sessions\")\n",
        "print(f\"   - Cross-Subject Cross-Experiment Accuracy: {accuracy*100:.2f}%\")\n",
        "print(f\"   - Note: This is a very challenging task as the model was trained on different subjects and experiments\")\n",
        "\n",
        "print(f\"\\n2. ALTERNATIVE APPROACH (Cross-Subject Generalization):\")\n",
        "print(f\"   - Training: Subjects {TRAIN_SUBJECTS_ALT}, Experiments [1,2], All sessions\")\n",
        "print(f\"   - Testing: Subjects {TEST_SUBJECTS_ALT}, Experiments [1,2], All sessions\")\n",
        "print(f\"   - Cross-Subject Test Accuracy: {test_acc_alt*100:.2f}%\")\n",
        "print(f\"   - Note: This tests the model's ability to generalize to completely unseen subjects\")\n",
        "\n",
        "print(f\"\\n3. KEY INSIGHTS:\")\n",
        "print(f\"   - Cross-subject generalization is inherently challenging in EEG-based biometrics\")\n",
        "print(f\"   - Different subjects have unique brain patterns, making direct transfer difficult\")\n",
        "print(f\"   - Cross-experiment testing (different experimental conditions) adds another layer of complexity\")\n",
        "print(f\"   - The alternative approach shows better performance as it uses the same experimental conditions\")\n",
        "print(f\"   - For real-world applications, domain adaptation techniques might be needed\")\n",
        "\n",
        "print(f\"\\n4. RECOMMENDATIONS:\")\n",
        "print(f\"   - For better cross-subject performance, consider:\")\n",
        "print(f\"     * Domain adaptation techniques\")\n",
        "print(f\"     * Transfer learning approaches\")\n",
        "print(f\"     * Subject-specific fine-tuning\")\n",
        "print(f\"     * Ensemble methods combining multiple models\")\n",
        "print(f\"     * Feature normalization techniques\")\n",
        "print(\"=\"*60)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
